{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [문제1] Fashion MNIST 데이터 정규화를 위한 평균, 표준편차 구하기\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "904671fd991932a7"
   },
   "id": "904671fd991932a7"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jv6EQKVfC7vA",
    "outputId": "95d0d71c-bc91-4e25-dc9a-986b9ad20597"
   },
   "id": "jv6EQKVfC7vA",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install wandb"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlwLeabeLZRB",
    "outputId": "0cdedd4e-6a57-4e19-ca49-e97b2c6f5c79"
   },
   "id": "OlwLeabeLZRB",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m31.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m190.6/190.6 kB\u001B[0m \u001B[31m28.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.34.0-py2.py3-none-any.whl (243 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m243.9/243.9 kB\u001B[0m \u001B[31m30.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.7/62.7 kB\u001B[0m \u001B[31m9.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=9ce218ac1c354d56d550d6a260143ba504584ff7ad7805b0074e39320c942543\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 pathtools-0.1.2 sentry-sdk-1.34.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transforms\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# BASE_PATH = '/content/drive/MyDrive/Colab Notebooks'\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mBASE_PATH\u001B[49m)\n\u001B[0;32m     14\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mappend(BASE_PATH)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m_01_code\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_99_common_utils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_num_cpu_cores, is_linux, is_windows\n",
      "\u001B[1;31mNameError\u001B[0m: name 'BASE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import Subset\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "BASE_PATH = '.'\n",
    "# BASE_PATH = '/content/drive/MyDrive/Colab Notebooks'\n",
    "print(BASE_PATH)\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "from _01_code._99_common_utils.utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "    f_train_mean, f_train_std = get_statistic(f_mnist_train)\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "    print(\"Label Shape: \", type(f_mnist_train[0][1]))\n",
    "    print(\"Mean: \", f_train_mean, \", Std: \", f_train_std)\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=f_train_mean, std=f_train_std),\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "    f_test_mean, f_test_std = get_statistic(f_mnist_test)\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "    print(\"Mean: \", f_test_mean, \", Std: \", f_test_std)\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=f_test_mean, std=f_test_std),\n",
    "    )\n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_statistic(subset: Subset):\n",
    "    dataset = torch.stack([t for t, _ in subset], dim=3)\n",
    "    mean = dataset.view(-1).mean().item()\n",
    "    std = dataset.view(-1).std().item()\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {'batch_size': 2048, }\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    print()\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "15bab1a99aa7b54e",
    "outputId": "f60762c6-9c2f-471f-dadd-49b51f97d6fe",
    "ExecuteTime": {
     "end_time": "2023-11-17T12:16:15.305276800Z",
     "start_time": "2023-11-17T12:16:09.990769100Z"
    }
   },
   "id": "15bab1a99aa7b54e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [문제 2] Fashion MNIST 데이터에 대하여 CNN 학습시키기"
   ],
   "metadata": {
    "id": "T9752RUgLL8a"
   },
   "id": "T9752RUgLL8a"
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "from datetime import datetime\n",
    "from _01_code._08_diverse_techniques.a_arg_parser import get_parser\n",
    "from _01_code._06_fcn_best_practice.c_trainer import EarlyStopping\n",
    "from _01_code._99_common_utils.utils import strfdelta\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(BASE_PATH, \"checkpoints\")\n",
    "if not os.path.isdir(CHECKPOINT_PATH):\n",
    "    os.makedirs(os.path.join(BASE_PATH, \"checkpoints\"))\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "            self, project_name, model, optimizer, train_data_loader, validation_data_loader,\n",
    "            transforms, run_time_str, wandb, device, checkpoint_file_path\n",
    "    ):\n",
    "        self.project_name = project_name\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.validation_data_loader = validation_data_loader\n",
    "        self.transforms = transforms\n",
    "        self.run_time_str = run_time_str\n",
    "        self.wandb = wandb\n",
    "        self.device = device\n",
    "        self.checkpoint_file_path = checkpoint_file_path\n",
    "        # reduction='mean' ensures loss_fn to produce scalar output (although not required)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    def run(self):\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=self.wandb.config.early_stop_patience,\n",
    "            delta=self.wandb.config.early_stop_delta,\n",
    "            project_name=self.project_name,\n",
    "            checkpoint_file_path=self.checkpoint_file_path,\n",
    "            run_time_str=self.run_time_str\n",
    "        )\n",
    "        n_epochs = self.wandb.config.epochs\n",
    "        training_start_time = datetime.now()\n",
    "    \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss, train_accuracy = self.train_model()\n",
    "    \n",
    "            if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "                validation_loss, validation_accuracy = self.validate_model()\n",
    "    \n",
    "                elapsed_time = datetime.now() - training_start_time\n",
    "                epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "    \n",
    "                message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "    \n",
    "                print(\n",
    "                    f\"[Epoch {epoch:>3}] \"\n",
    "                    f\"T_loss: {train_loss:7.5f}, \"\n",
    "                    f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "                    f\"V_loss: {validation_loss:7.5f}, \"\n",
    "                    f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "                    f\"{message} | \"\n",
    "                    f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "                    f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "                )\n",
    "    \n",
    "                self.wandb.log({\n",
    "                    \"Epoch\": epoch,\n",
    "                    \"Training loss\": train_loss,\n",
    "                    \"Training accuracy (%)\": train_accuracy,\n",
    "                    \"Validation loss\": validation_loss,\n",
    "                    \"Validation accuracy (%)\": validation_accuracy,\n",
    "                    \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "                })\n",
    "    \n",
    "                if early_stop:\n",
    "                    break\n",
    "    \n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")\n",
    "\n",
    "    def train_model(self):\n",
    "        self.model.train()\n",
    "        loss_sum = 0.0\n",
    "        correct_num = 0\n",
    "        trained_sample_num = 0\n",
    "        train_num = 0\n",
    "\n",
    "        for batch in self.train_data_loader:\n",
    "            input_tensor, target_tensor = batch\n",
    "            input_tensor.to(self.device)\n",
    "            target_tensor.to(self.device)\n",
    "            input_tensor = self.transforms(input_tensor)\n",
    "            output_tensor = self.model(input_tensor)\n",
    "            loss = self.loss_fn(output_tensor, target_tensor)\n",
    "            loss_sum += loss.item()  # aggregate loss (assert it's scalar)\n",
    "\n",
    "            # decode one-hot prediction\n",
    "            predicted_tensor = torch.argmax(output_tensor, dim=1)\n",
    "            correct_num += torch.sum(torch.eq(predicted_tensor, target_tensor)).item()\n",
    "            trained_sample_num += len(input_tensor)\n",
    "            train_num += 1\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()  # compute gradients\n",
    "            self.optimizer.step()  # update parameters\n",
    "\n",
    "        train_loss = loss_sum / train_num\n",
    "        train_accuracy = 100.0 * correct_num / trained_sample_num\n",
    "\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "    def validate_model(self):\n",
    "        self.model.eval()\n",
    "        loss_sum = 0.0\n",
    "        correct_num = 0\n",
    "        validated_sample_num = 0\n",
    "        validate_num = 0\n",
    "\n",
    "        # optimization is not required\n",
    "        with torch.no_grad():\n",
    "            for validation_batch in self.validation_data_loader:\n",
    "                input_tensor, target_tensor = validation_batch\n",
    "                input_tensor = input_tensor.to(device=self.device)\n",
    "                target_tensor = target_tensor.to(device=self.device)\n",
    "                input_tensor = self.transforms(input_tensor)\n",
    "                output_tensor = self.model(input_tensor)\n",
    "                loss_sum += self.loss_fn(output_tensor, target_tensor).item()\n",
    "\n",
    "                predicted_tensor = torch.argmax(output_tensor, dim=1)\n",
    "                correct_num += torch.sum(torch.eq(predicted_tensor, target_tensor)).item()\n",
    "\n",
    "                validated_sample_num += len(input_tensor)\n",
    "                validate_num += 1\n",
    "\n",
    "        validation_loss = loss_sum / validate_num\n",
    "        validation_accuracy = 100.0 * correct_num / validated_sample_num\n",
    "\n",
    "        return validation_loss, validation_accuracy\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'validation_intervals': args.validation_intervals,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'early_stop_patience': args.early_stop_patience,\n",
    "        'early_stop_delta': args.early_stop_delta,\n",
    "        'weight_decay': args.weight_decay,\n",
    "        'dropout': args.dropout,\n",
    "        'normalization': args.normalization,\n",
    "        'augment': args.augment,\n",
    "    }\n",
    "\n",
    "    if args.augment:\n",
    "        augment_name = \"image_augment\"\n",
    "    else:\n",
    "        augment_name = \"no_image_augment\"\n",
    "\n",
    "    run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    name = \"{0}_{1}\".format(augment_name, run_time_str)\n",
    "\n",
    "    project_name = \"hw3_fashion_mnist\"\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=project_name,\n",
    "        notes=\"fashion mnist dataset\",\n",
    "        tags=[\"cnn\", \"fashion_mnist\", \"image_augment\"],\n",
    "        name=name,\n",
    "        config=config\n",
    "    )\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on device {device}.\")\n",
    "\n",
    "    if wandb.config.augment:\n",
    "        train_data_loader, validation_data_loader, transforms = get_augmented_fashion_mnist_data()\n",
    "    else:\n",
    "        train_data_loader, validation_data_loader, transforms = get_fashion_mnist_data()\n",
    "\n",
    "    model = get_fashion_mnist_model()\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    optimizers = [\n",
    "        optim.SGD(model.parameters(), lr=wandb.config.learning_rate, weight_decay=args.weight_decay),\n",
    "        optim.SGD(model.parameters(), lr=wandb.config.learning_rate, momentum=0.9, weight_decay=args.weight_decay),\n",
    "        optim.RMSprop(model.parameters(), lr=wandb.config.learning_rate, weight_decay=args.weight_decay),\n",
    "        optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=args.weight_decay)\n",
    "    ]\n",
    "\n",
    "    print(\"Optimizer:\", optimizers[args.optimizer])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        project_name, model, optimizers[args.optimizer],\n",
    "        train_data_loader, validation_data_loader, transforms,\n",
    "        run_time_str, wandb, device, CHECKPOINT_PATH\n",
    "    )\n",
    "    trainer.run()\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "def get_augmented_fashion_mnist_data():\n",
    "    # todo augment the dataset\n",
    "    return get_fashion_mnist_data()\n",
    "\n",
    "\n",
    "def get_fashion_mnist_model():\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self, in_channel, n_output):\n",
    "            super().__init__()\n",
    "\n",
    "            self.model = nn.Sequential(\n",
    "                # B x 1 x 28 x 28 --> B x 6 x (28 - 5 + 1) x (28 - 5 + 1) = B x 6 x 24 x 24\n",
    "                nn.Conv2d(in_channels=in_channel, out_channels=6, kernel_size=(5, 5), stride=(1, 1)),\n",
    "                # B x 6 x 24 x 24 --> B x 6 x 12 x 12\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "                # B x 6 x 12 x 12 --> B x 16 x (12 - 5 + 1) x (12 - 5 + 1) = B x 16 x 8 x 8\n",
    "                nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5), stride=(1, 1)),\n",
    "                # B x 16 x 8 x 8 --> B x 16 x 4 x 4\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(128, n_output),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    return MyModel(in_channel=1, n_output=10)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ],
   "metadata": {
    "id": "f8sqGehE603r",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3f5cafcf-a93c-4ecb-a06f-53c249f86458"
   },
   "id": "f8sqGehE603r",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
